{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PipelineStreaming.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "zl61Jc5p2Ye4",
        "v0v6dkU0SVKs",
        "d602WN_7AB76",
        "2CCNH0KiCo1k",
        "zXAUvyisPvtd"
      ],
      "mount_file_id": "1dob6549q13vz_zqsQAbR4bCx-TjRAwZw",
      "authorship_tag": "ABX9TyMd9yLjBzSoOUXTQCwfDc4/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/felipecampelo/PipelineStreaming/blob/main/PipelineStreaming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✅ Pipeline de Streaming de Dados ✅\n",
        "\n",
        "`Objetivo`: Gerar dados streaming para o tópico 01 (Pub/Sub 01), executar a pipeline de dados streaming com o Dataflow (com o modelo gerado pelo Colab) que vai transferir os dados do tópico 01 para o tópico 02 (Pub/Sub 02), consumir os dados do tópico 02."
      ],
      "metadata": {
        "id": "8tcb_gpXcGKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ❗ Apache Beam ❗\n",
        "\n",
        "`Passo a passo para instalação do Apache Beam no Colab:`\n",
        "\n",
        "`1)` pip install --upgrade pip\n",
        "\n",
        "`2)` pip install apache-beam[interactive]\n",
        "\n",
        "`3)` Reiniciar ambiente de execução\n",
        "\n",
        "`4)` pip install apache-beam[gcp]\n",
        "\n",
        "`5)` Reiniciar ambiente de execução\n",
        "\n",
        "`6)` import apache_beam as beam\n",
        "\n",
        "`OBS: Nunca usar funções de exibição e gravação na mesma pipeline`"
      ],
      "metadata": {
        "id": "zl61Jc5p2Ye4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3VGZyuQupOi"
      },
      "outputs": [],
      "source": [
        "# Necessário atualizar o pip para instalação do Beam\n",
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o apache-beam[interactive]\n",
        "!pip install apache-beam[interactive]"
      ],
      "metadata": {
        "id": "TLgNr3JE022K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando o apache-beam[gcp]\n",
        "!pip install apache-beam[gcp]"
      ],
      "metadata": {
        "id": "1QE7LYPn1vn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando a biblioteca do Pub/Sub do GCP\n",
        "!pip install google-cloud-pubsub"
      ],
      "metadata": {
        "id": "uD4nBauESw3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###⚡ Importando bibliotecas necessárias ⚡\n"
      ],
      "metadata": {
        "id": "v0v6dkU0SVKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importando as bibliotecas necessárias\n",
        "import csv\n",
        "import time\n",
        "from google.cloud import pubsub_v1\n",
        "import os\n",
        "\n",
        "# Bibliotecas para Pipeline\n",
        "import apache_beam as beam\n",
        "import os\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "from apache_beam import window # Para trabalhar a pipeline com streaming"
      ],
      "metadata": {
        "id": "zD14BCDMSbFe"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###⚡ Simulador de Streaming de Dados e Processamento via Pub/Sub ⚡\n",
        "\n",
        "`Pub: Enche o tópico de dados`\n",
        "\n",
        "`Sub: Lê e envia os dados do tópico`"
      ],
      "metadata": {
        "id": "d602WN_7AB76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conectando com o service account do GCP\n",
        "serviceAccount = '/content/drive/MyDrive/KeysGCP/aulas-soulcode-felipe-1ab7e143ccf1.json'\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = serviceAccount"
      ],
      "metadata": {
        "id": "MUPp1E6TPjBU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⏩ Producer ou Criador de Dados (PUB) ⏪"
      ],
      "metadata": {
        "id": "2CCNH0KiCo1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o topico e o publisher\n",
        "topico = 'projects/aulas-soulcode-felipe/topics/igorconsumer01'\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "\n",
        "# Definindo os dados de entrada\n",
        "entrada = '/content/drive/MyDrive/Datasets/voos.csv'\n",
        "\n",
        "# Enviando os dados do CSV em linha a linha (simulando Streaming)\n",
        "with open(entrada, 'rb') as file:\n",
        "  next(file) # Pulando a linha das colunas\n",
        "  for row in file:\n",
        "    print('Linha enviada')\n",
        "    publisher.publish(topico, row)"
      ],
      "metadata": {
        "id": "EttxuCV-AWmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ⏩ Consumer ou Extrator de Dados (SUB) ⏪"
      ],
      "metadata": {
        "id": "zXAUvyisPvtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo a subscription e o subscriber\n",
        "subscription = 'projects/aulas-soulcode-felipe/subscriptions/igorsub02'\n",
        "subscriber = pubsub_v1.SubscriberClient()\n",
        "\n",
        "# Para ler as mensagens e assim poder apagar do topico\n",
        "def visualizarMsg(mensagem):\n",
        "  print(('mensagem: {}'.format(mensagem)))\n",
        "  mensagem.ack()\n",
        "\n",
        "subscriber.subscribe(subscription, callback = visualizarMsg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhCsqcroMlSr",
        "outputId": "bfa2733b-7f4a-4704-9d8f-8a416b182479"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<StreamingPullFuture at 0x7f61676fec50 state=pending>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###⚡ Modelo de Pipeline para Streaming ⚡"
      ],
      "metadata": {
        "id": "vDWUc1HrZ5YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criando as configurações da Pipeline para conexão com o GCP\n",
        "pipeline_options = {\n",
        "    'project': 'aulas-soulcode-felipe', # ID do projeto do GCP\n",
        "    'runner': 'DataflowRunner', # Aplicação que irá rodar (Dataflow)\n",
        "    'region': 'southamerica-east1', # Região de preferência\n",
        "    'staging_location': 'gs://atividade-beam-dataflow/staging/', # Localização para arquivos em staging\n",
        "    'temp_location': 'gs://atividade-beam-dataflow/temp/', # Localização para arquivos temporários\n",
        "    'template_location': 'gs://atividade-beam-dataflow/models/modelo_streaming', # Localização do modelo de Pipeline\n",
        "    'save_main_session': True,\n",
        "    'streaming': True\n",
        "}\n",
        "\n",
        "# Transformando o dicionário para o tipo de PipelineOption\n",
        "pipeline_options = PipelineOptions.from_dictionary(pipeline_options)\n",
        "\n",
        "# Criando a Pipeline com as opções estabelecidas\n",
        "p1 = beam.Pipeline(options = pipeline_options)\n",
        "\n",
        "# Definindo a entrada e saída do Pipeline de acordo com o objetivo\n",
        "entrada = 'projects/aulas-soulcode-felipe/subscriptions/igorsub01' # Subscription do Pub/Sub 01\n",
        "saida = 'projects/aulas-soulcode-felipe/topics/igorconsumer02' # Tópico do Pub/Sub 02\n",
        "\n",
        "# Classe com uma função para que cada registro seja decodificado para utf-8 e separado por vírgula\n",
        "# Motivo: Pub/Sub entende os dados bit a bit\n",
        "# Alternativa: Podemos também apenas colocar o return dessa função no beam.Map (não usando ParDo)\n",
        "class separador(beam.DoFn):\n",
        "  def process(self, record):\n",
        "    return [record.decode('utf-8').split(',')]\n",
        "\n",
        "# Perna para coletar os dados de entrada\n",
        "pcollection_entrada = (\n",
        "    p1 \n",
        "    |'Ler do topico' >> beam.io.ReadFromPubSub(subscription = entrada)\n",
        ")\n",
        "\n",
        "tempoAtraso = (\n",
        "    pcollection_entrada\n",
        "    |'Separador 1 do Tópico - CSV' >> beam.ParDo(separador()) # Definindo o separador do arquivo\n",
        "    |'Filtrar atrasos maior que zero (1)' >> beam.Filter(lambda record: float(record[8]) > 0) # Filtrando os que tem atraso\n",
        "    |'Agregar as colunas (1)' >> beam.Map(lambda record: (record[0], float(record[1]))) # Pegando apenas as colunas 4 e 8\n",
        "    |'Janela de sliding (1)' >> beam.WindowInto(window.SlidingWindows(15, 10)) # Definindo a window -> 15 é o tempo total da janela, 10 é o momento que abre outra\n",
        "    |'Construir uma nova tabela por minuto (1)' >> beam.CombinePerKey(sum) # Agrupando por chave e fazendo a soma\n",
        ")\n",
        "\n",
        "qtdAtraso = (\n",
        "    pcollection_entrada\n",
        "    |'Separador 2 do Tópico - CSV' >> beam.ParDo(separador()) # Definindo o separador do arquivo\n",
        "    |'Filtrar atrasos maior que zero (2)' >> beam.Filter(lambda record: float(record[8]) > 0) # Filtrando os que tem atraso\n",
        "    |'Agregar as colunas (2)' >> beam.Map(lambda record: (record[0], float(record[1]))) # Pegando apenas as colunas 4 e 8\n",
        "    |'Janela de sliding (2)' >> beam.WindowInto(window.SlidingWindows(15, 10)) # Definindo a window -> 15 é o tempo total da janela, 10 é o momento que abre outra\n",
        "    |'Construir uma nova tabela por contagem (2)' >> beam.combiners.Count.PerKey() # Fazendo uma contagem de ocorrências\n",
        ")\n",
        "\n",
        "# A última pipeline não precisa do p1 (pernas) já que estamos juntando elas\n",
        "tabela = (\n",
        "    {'Quantidade_minutos': tempoAtraso, 'Numero_de_atrasos': qtdAtraso}\n",
        "    |'Agrupar as pernas' >> beam.CoGroupByKey()\n",
        "    |'Converter para PubSub' >> beam.Map(lambda row: (''.join(str(row)).encode('utf-8')))\n",
        "    |'Gravar o resultado' >> beam.io.WriteToPubSub(saida) # Enviando para o Pub/Sub\n",
        ")\n",
        "\n",
        "# Executando as Pipelines\n",
        "execucao = p1.run()\n",
        "execucao.wait_until_finish() # Para esperar até finalizar (não sabemos o fim do streaming)"
      ],
      "metadata": {
        "id": "TE4yfUlFZ9R3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}